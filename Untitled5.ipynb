{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgXSLJt+qGo4hPjaKb9WBs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnu-priya28/Brain-tumor-using-SVM-CNN-AND-BCM-algorithms/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sxaZSUtIF8D",
        "outputId": "73f6c86b-9689-4d02-9741-090b762f25af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import seaborn as sns\n",
        "import os\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "PYPTa9DkLNFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Replace backslashes with forward slashes in the path\n",
        "directory_path = '/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(directory_path):\n",
        "    # Walk through the directory and print file paths\n",
        "    for dirname, _, filenames in os.walk(directory_path):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(dirname, filename)\n",
        "            print(file_path)\n",
        "else:\n",
        "    print(f\"The directory '{directory_path}' does not exist.\")\n"
      ],
      "metadata": {
        "id": "OY8-Ani-LNKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48771584-0ec8-4641-9968-c52280f80bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y4.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y160.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y45.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y85.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y75.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y117.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y109.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y29.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y193.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y51.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y37.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y159.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y95.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y11.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y256.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y161.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y157.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y96.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y167.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y102.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y16.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y112.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y254.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y187.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y100.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y182.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y192.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y170.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y30.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y33.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y41.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y54.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y253.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y194.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y28.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y21.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y86.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y106.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y19.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y120.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y53.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y249.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y98.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y147.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y70.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y148.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y186.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y243.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y107.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y164.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y111.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y7.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y251.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y244.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y97.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y259.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y9.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y116.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y36.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y18.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y101.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y39.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y22.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y32.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y65.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y242.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y56.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y44.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y66.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y91.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y47.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y188.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y158.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y113.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y46.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y40.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y69.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y58.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y14.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y90.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y79.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y3.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y71.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y183.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y103.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y59.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y247.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y248.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y55.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y104.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y23.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y10.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y61.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y166.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y115.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y67.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y162.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y180.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y195.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y153.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y52.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y146.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y73.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y82.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y108.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y89.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y34.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y181.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y15.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y257.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y38.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y35.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y168.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y1.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y154.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y165.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y49.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y169.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y99.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y245.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y62.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y20.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y24.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y105.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y42.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y77.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y74.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y92.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y114.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y6.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y156.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y163.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y76.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y92.png\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y258.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y60.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y246.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y155.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y185.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y27.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y13.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y8.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y78.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y25.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y2.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y12.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y26.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y31.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y250.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y255.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y252.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y184.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y81.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y50.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y17.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/11 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/10 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/1 no.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/44no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/18 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/41 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/12 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/27 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/23 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/48 no.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/33 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/13 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/9 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/31 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N20.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/50 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/37 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/3 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/21 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/38 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/42 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/28 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/17 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/32 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/49 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/19 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/30 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N17.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/43 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N16.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N1.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/40 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/7 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/45 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/26 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/36 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/24 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/8 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N21.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N11.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/39 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N22.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/4 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/20 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N2.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/47 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/34 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/22 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/29 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N19.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/14 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N15.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/25 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/35 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/15 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/6 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/2 no.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/46 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/5 no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 96.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 8.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 2.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No15.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 95.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N5.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 97.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N6.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 9.png\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 90.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 98.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 92.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 4.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No12.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 1.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No20.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 89.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No22.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 91.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N3.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 3.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No11.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/N26.JPG\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No17.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No21.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 7.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 6.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No16.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 923.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No19.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 5.jpeg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 99.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No13.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No14.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No18.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 10.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 94.jpg\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 100.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_data(data_folder):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for subfolder in ['train', 'valid']:\n",
        "        for folder in ['yes', 'no']:\n",
        "            label = 1 if folder == 'yes' else 0\n",
        "            folder_path = os.path.join(data_folder, subfolder, folder)\n",
        "\n",
        "            for filename in os.listdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, filename)\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.resize(image, (150, 150))  # Resize images to a consistent size\n",
        "\n",
        "                features.append(image)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load and preprocess data\n",
        "data_folder = '/content/drive/MyDrive/btumor project/brain/dataset'\n",
        "X, y = load_and_preprocess_data(data_folder)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Build the BCM-CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation on training data\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=30, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Extract features from the BCM-CNN model for training and validation sets\n",
        "bcm_cnn_features_train = model.predict(X_train)\n",
        "bcm_cnn_features_valid = model.predict(X_valid)\n",
        "\n",
        "# Reshape BCM-CNN features for compatibility with SVM\n",
        "bcm_cnn_features_train_reshaped = bcm_cnn_features_train.reshape(bcm_cnn_features_train.shape[0], -1)\n",
        "bcm_cnn_features_valid_reshaped = bcm_cnn_features_valid.reshape(bcm_cnn_features_valid.shape[0], -1)\n",
        "\n",
        "# Train SVM on BCM-CNN features\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(bcm_cnn_features_train_reshaped, y_train)\n",
        "\n",
        "# Extract features from the BCM-CNN model for the validation set\n",
        "bcm_cnn_features_valid = model.predict(X_valid)\n",
        "bcm_cnn_features_valid_reshaped = bcm_cnn_features_valid.reshape(bcm_cnn_features_valid.shape[0], -1)\n",
        "\n",
        "# Make predictions using SVM on BCM-CNN features for validation set\n",
        "svm_predictions_valid = svm_model.predict(bcm_cnn_features_valid_reshaped)\n",
        "\n",
        "# Evaluate the combined model on validation set\n",
        "combined_accuracy_valid = accuracy_score(y_valid, svm_predictions_valid)\n",
        "print(f'Combined Model Accuracy on Validation Set: {combined_accuracy_valid * 100:.2f}%')\n",
        "#Combined Model Accuracy on Validation Set: 83.67%\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRGA0TBfUEL",
        "outputId": "26a1717f-3be7-40bf-917c-7b21bce3e3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "75/75 [==============================] - 150s 2s/step - loss: 0.5886 - accuracy: 0.6833 - val_loss: 86.9170 - val_accuracy: 0.7733\n",
            "Epoch 2/30\n",
            "75/75 [==============================] - 146s 2s/step - loss: 0.4853 - accuracy: 0.7733 - val_loss: 35.9480 - val_accuracy: 0.8033\n",
            "Epoch 3/30\n",
            "75/75 [==============================] - 143s 2s/step - loss: 0.4286 - accuracy: 0.8146 - val_loss: 74.1433 - val_accuracy: 0.7567\n",
            "Epoch 4/30\n",
            "75/75 [==============================] - 155s 2s/step - loss: 0.3673 - accuracy: 0.8438 - val_loss: 62.2703 - val_accuracy: 0.7983\n",
            "Epoch 5/30\n",
            "75/75 [==============================] - 144s 2s/step - loss: 0.3302 - accuracy: 0.8596 - val_loss: 57.6252 - val_accuracy: 0.7767\n",
            "Epoch 6/30\n",
            "75/75 [==============================] - 144s 2s/step - loss: 0.3007 - accuracy: 0.8838 - val_loss: 71.0089 - val_accuracy: 0.7967\n",
            "Epoch 7/30\n",
            "75/75 [==============================] - 143s 2s/step - loss: 0.2731 - accuracy: 0.8904 - val_loss: 88.1200 - val_accuracy: 0.7550\n",
            "Epoch 8/30\n",
            "75/75 [==============================] - 141s 2s/step - loss: 0.2617 - accuracy: 0.8917 - val_loss: 58.2405 - val_accuracy: 0.7683\n",
            "Epoch 9/30\n",
            "75/75 [==============================] - 141s 2s/step - loss: 0.1987 - accuracy: 0.9217 - val_loss: 62.0342 - val_accuracy: 0.8233\n",
            "Epoch 10/30\n",
            "75/75 [==============================] - 140s 2s/step - loss: 0.1792 - accuracy: 0.9375 - val_loss: 16.9788 - val_accuracy: 0.9250\n",
            "Epoch 11/30\n",
            "75/75 [==============================] - 139s 2s/step - loss: 0.1599 - accuracy: 0.9379 - val_loss: 20.9131 - val_accuracy: 0.8983\n",
            "Epoch 12/30\n",
            "75/75 [==============================] - 139s 2s/step - loss: 0.1339 - accuracy: 0.9496 - val_loss: 32.8645 - val_accuracy: 0.8817\n",
            "Epoch 13/30\n",
            "75/75 [==============================] - 138s 2s/step - loss: 0.1305 - accuracy: 0.9492 - val_loss: 22.3788 - val_accuracy: 0.9150\n",
            "Epoch 14/30\n",
            "75/75 [==============================] - 139s 2s/step - loss: 0.1149 - accuracy: 0.9621 - val_loss: 25.6816 - val_accuracy: 0.9283\n",
            "Epoch 15/30\n",
            "75/75 [==============================] - 138s 2s/step - loss: 0.1068 - accuracy: 0.9588 - val_loss: 55.2787 - val_accuracy: 0.8550\n",
            "Epoch 16/30\n",
            "75/75 [==============================] - 138s 2s/step - loss: 0.0752 - accuracy: 0.9775 - val_loss: 32.1355 - val_accuracy: 0.9217\n",
            "Epoch 17/30\n",
            "75/75 [==============================] - 139s 2s/step - loss: 0.0815 - accuracy: 0.9679 - val_loss: 29.3000 - val_accuracy: 0.9317\n",
            "Epoch 18/30\n",
            "75/75 [==============================] - 140s 2s/step - loss: 0.0803 - accuracy: 0.9729 - val_loss: 17.2698 - val_accuracy: 0.9467\n",
            "Epoch 19/30\n",
            "75/75 [==============================] - 138s 2s/step - loss: 0.0766 - accuracy: 0.9750 - val_loss: 46.5205 - val_accuracy: 0.8683\n",
            "Epoch 20/30\n",
            "75/75 [==============================] - 142s 2s/step - loss: 0.0596 - accuracy: 0.9767 - val_loss: 51.2748 - val_accuracy: 0.9067\n",
            "Epoch 21/30\n",
            "75/75 [==============================] - 140s 2s/step - loss: 0.0432 - accuracy: 0.9846 - val_loss: 66.6207 - val_accuracy: 0.8600\n",
            "Epoch 22/30\n",
            "75/75 [==============================] - 142s 2s/step - loss: 0.0579 - accuracy: 0.9783 - val_loss: 67.5281 - val_accuracy: 0.8783\n",
            "Epoch 23/30\n",
            "75/75 [==============================] - 132s 2s/step - loss: 0.0615 - accuracy: 0.9767 - val_loss: 71.9290 - val_accuracy: 0.8717\n",
            "Epoch 24/30\n",
            "75/75 [==============================] - 132s 2s/step - loss: 0.0391 - accuracy: 0.9908 - val_loss: 59.1807 - val_accuracy: 0.8967\n",
            "Epoch 25/30\n",
            "75/75 [==============================] - 132s 2s/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 76.0554 - val_accuracy: 0.9017\n",
            "Epoch 26/30\n",
            "75/75 [==============================] - 133s 2s/step - loss: 0.0436 - accuracy: 0.9854 - val_loss: 99.6183 - val_accuracy: 0.8533\n",
            "Epoch 27/30\n",
            "75/75 [==============================] - 134s 2s/step - loss: 0.0456 - accuracy: 0.9808 - val_loss: 86.3503 - val_accuracy: 0.8917\n",
            "Epoch 28/30\n",
            "75/75 [==============================] - 136s 2s/step - loss: 0.0364 - accuracy: 0.9875 - val_loss: 58.5352 - val_accuracy: 0.9000\n",
            "Epoch 29/30\n",
            "75/75 [==============================] - 132s 2s/step - loss: 0.0478 - accuracy: 0.9867 - val_loss: 37.1533 - val_accuracy: 0.9117\n",
            "Epoch 30/30\n",
            "75/75 [==============================] - 132s 2s/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 54.1878 - val_accuracy: 0.8950\n",
            "75/75 [==============================] - 33s 441ms/step\n",
            "19/19 [==============================] - 7s 382ms/step\n",
            "19/19 [==============================] - 9s 457ms/step\n",
            "Combined Model Accuracy on Validation Set: 89.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\n",
        "\n",
        "# Evaluate the combined model on validation set\n",
        "combined_accuracy_valid = accuracy_score(y_valid, svm_predictions_valid)\n",
        "combined_precision_valid = precision_score(y_valid, svm_predictions_valid)\n",
        "combined_recall_valid = recall_score(y_valid, svm_predictions_valid)\n",
        "combined_f1_score_valid = f1_score(y_valid, svm_predictions_valid)\n",
        "\n",
        "# Print the accuracy, precision, recall, and F1-score\n",
        "print(f'Combined Model Accuracy on Validation Set: {combined_accuracy_valid * 100:.2f}%')\n",
        "print(f'Combined Model Precision on Validation Set: {combined_precision_valid:.2f}')\n",
        "print(f'Combined Model Recall on Validation Set: {combined_recall_valid:.2f}')\n",
        "print(f'Combined Model F1-Score on Validation Set: {combined_f1_score_valid:.2f}')\n",
        "\n",
        "# Create and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_valid, svm_predictions_valid)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMAmAiqRJ4Af",
        "outputId": "583b7447-bb41-4c27-d959-efb37b5abbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Model Accuracy on Validation Set: 89.50%\n",
            "Combined Model Precision on Validation Set: 0.85\n",
            "Combined Model Recall on Validation Set: 0.97\n",
            "Combined Model F1-Score on Validation Set: 0.91\n",
            "Confusion Matrix:\n",
            "[[229  53]\n",
            " [ 10 308]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Function to preprocess new images\n",
        "def preprocess_new_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, (150, 150))  # Resize images to match the model input size\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load SVM model\n",
        "svm_model_path = '/content/drive/MyDrive/btumor project/svm_model(1).pkl'\n",
        "svm_model = joblib.load(svm_model_path)\n",
        "\n",
        "# List of paths to new images\n",
        "new_image_paths = ['/content/drive/MyDrive/btumor project/brain/dataset/test/N1.jpeg','/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/15 no.jpg','/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y109.JPG','/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y148.JPG','/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 96.jpg', '/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No17.jpg', '/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y10.jpg','/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y51.jpg']\n",
        "\n",
        "# Preprocess new images\n",
        "new_images = preprocess_new_images(new_image_paths)\n",
        "\n",
        "# Extract features from the BCM-CNN model for new images\n",
        "bcm_cnn_features_new = model.predict(new_images)\n",
        "bcm_cnn_features_new_reshaped = bcm_cnn_features_new.reshape(bcm_cnn_features_new.shape[0], -1)\n",
        "\n",
        "# Make predictions using SVM on BCM-CNN features for new images\n",
        "svm_predictions_new = svm_model.predict(bcm_cnn_features_new_reshaped)\n",
        "\n",
        "# Print the predictions for new images\n",
        "for image_path, prediction in zip(new_image_paths, svm_predictions_new):\n",
        "    result = 'Tumor' if prediction == 1 else 'No Tumor'\n",
        "    print(f'{image_path}: {result}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swtS4U9A0uDe",
        "outputId": "99837da2-cc30-4e57-adc5-7fc4eebdeb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 241ms/step\n",
            "/content/drive/MyDrive/btumor project/brain/dataset/test/N1.jpeg: No Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/15 no.jpg: No Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y109.JPG: Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y148.JPG: Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/no 96.jpg: No Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/no/No17.jpg: No Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y10.jpg: Tumor\n",
            "/content/drive/MyDrive/btumor project/brain_tumor_dataset(1)/yes/Y51.jpg: Tumor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "END OF IT"
      ],
      "metadata": {
        "id": "_VNIdYwH2HV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEoH_Qdt2HZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mak446SN2HcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jvWV1cl02Hfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP_VnLGJ2Hi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hN-KdR9H2HmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfCZ5j3G2Hp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRlxMFXc2HuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJwNzkj82Hxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7oSaPGrP2HzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-o3gxDHS2H15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class (0 for no tumor, 1 for tumor)\n",
        "            labels.append(1 if class_label == 'tumor' else 0)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define the path to your dataset directory\n",
        "dataset_path = '/content/drive/MyDrive/brain_tumor_dataset(1)'\n",
        "\n",
        "# Load and label the data\n",
        "images, labels = load_and_label_data(dataset_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the shape of the training and testing sets\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "print(\"Testing labels shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "FYXT8oZ5LNNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeadf895-f86b-476d-8606-83b8b58e53f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (202, 128, 128)\n",
            "Testing data shape: (51, 128, 128)\n",
            "Training labels shape: (202,)\n",
            "Testing labels shape: (51,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class ('no' for no tumor, 'yes' for tumor)\n",
        "            labels.append('yes' if class_label == 'yes' else 'no')\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and label the data\n",
        "images, labels = load_and_label_data('/content/drive/MyDrive/brain_tumor_dataset(1)')\n",
        "\n",
        "# Convert labels to binary (0 for 'no', 1 for 'yes')\n",
        "binary_labels = (labels == 'yes').astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, binary_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# ... (The rest of the code remains unchanged)\n",
        "\n",
        "\n",
        "# Reshape the data to fit the input shape of the BCN model\n",
        "input_shape = (128, 128, 1)  # Adjust based on your actual image size and channels\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0],) + input_shape)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0],) + input_shape)\n",
        "\n",
        "# Build and train the BCN model\n",
        "bcn_model = models.Sequential()\n",
        "bcn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "bcn_model.add(layers.MaxPooling2D((2, 2)))\n",
        "bcn_model.add(layers.Flatten())\n",
        "bcn_model.add(layers.Dense(64, activation='relu'))\n",
        "bcn_model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "bcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bcn_model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# ...\n",
        "\n",
        "# Get the BCN model predictions on the test set\n",
        "bcn_predictions_test = bcn_model.predict(X_test_reshaped)\n",
        "bcn_predictions_test = (bcn_predictions_test > 0.5).astype(int)\n",
        "\n",
        "# Flatten BCN predictions to be used as features for SVM\n",
        "bcn_features_train = bcn_model.predict(X_train_reshaped).flatten()\n",
        "bcn_features_test = bcn_predictions_test.flatten()\n",
        "\n",
        "# Combine BCN features with the original features (flattened images) for SVM\n",
        "svm_features_train = np.concatenate([X_train.flatten(), bcn_features_train])\n",
        "svm_features_test = np.concatenate([X_test.flatten(), bcn_features_test])\n",
        "\n",
        "# Reshape SVM features to have the same number of samples as the original data\n",
        "svm_features_train = svm_features_train.reshape((X_train.shape[0], -1))\n",
        "svm_features_test = svm_features_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "# Build and train the SVM model with binary classification\n",
        "svm_model = SVC(kernel='linear', C=1, probability=True, class_weight='balanced', random_state=42)\n",
        "svm_model.fit(svm_features_train, y_train)\n",
        "\n",
        "# Get SVM predictions on the test set\n",
        "svm_predictions_test = svm_model.predict(svm_features_test)\n",
        "\n",
        "# Combine predictions from both models\n",
        "combined_predictions_test = np.round((svm_predictions_test + bcn_predictions_test) / 2).astype(int)\n",
        "\n",
        "# Evaluate the performance of each model and the combined model\n",
        "accuracy_bcn = accuracy_score(y_test, bcn_predictions_test)\n",
        "accuracy_svm = accuracy_score(y_test, svm_predictions_test)\n",
        "accuracy_combined = accuracy_score(y_test, combined_predictions_test)\n",
        "\n",
        "print(\"BCN Model Accuracy:\", accuracy_bcn)\n",
        "print(\"SVM Model Accuracy:\", accuracy_svm)\n",
        "print(\"Combined Model Accuracy:\", accuracy_combined)\n"
      ],
      "metadata": {
        "id": "_cpoTolMPOJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class (0 for no tumor, 1 for tumor)\n",
        "            labels.append(1 if class_label == 'tumor' else 0)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and label the data\n",
        "images, labels = load_and_label_data('/content/drive/MyDrive/brain_tumor_dataset(1)')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to fit the input shape of the BCN model\n",
        "input_shape = (128, 128, 1)  # Adjust based on your actual image size and channels\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0],) + input_shape)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0],) + input_shape)\n",
        "\n",
        "# Build and train the BCN model\n",
        "bcn_model = models.Sequential()\n",
        "bcn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "bcn_model.add(layers.MaxPooling2D((2, 2)))\n",
        "bcn_model.add(layers.Flatten())\n",
        "bcn_model.add(layers.Dense(64, activation='relu'))\n",
        "bcn_model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "bcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bcn_model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Get the BCN model predictions on the test set\n",
        "bcn_predictions_test = bcn_model.predict(X_test_reshaped)\n",
        "bcn_predictions_test = (bcn_predictions_test > 0.5).astype(int)\n",
        "\n",
        "# Flatten BCN predictions to be used as features for SVM\n",
        "bcn_features_train = bcn_model.predict(X_train_reshaped).flatten()\n",
        "bcn_features_test = bcn_predictions_test.flatten()\n",
        "\n",
        "# Combine BCN features with the original features (flattened images) for SVM\n",
        "svm_features_train = np.concatenate([X_train.flatten(), bcn_features_train])\n",
        "svm_features_test = np.concatenate([X_test.flatten(), bcn_features_test])\n",
        "\n",
        "# Reshape SVM features to have the same number of samples as the original data\n",
        "svm_features_train = svm_features_train.reshape((X_train.shape[0], -1))\n",
        "svm_features_test = svm_features_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "# Build and train the SVM model with binary classification\n",
        "svm_model = SVC(kernel='linear', C=1, probability=True, class_weight='balanced', random_state=42)\n",
        "svm_model.fit(svm_features_train, y_train)\n",
        "\n",
        "# Get SVM predictions on the test set\n",
        "svm_predictions_test = svm_model.predict(svm_features_test)\n",
        "\n",
        "# Combine predictions from both models\n",
        "combined_predictions_test = np.round((svm_predictions_test + bcn_predictions_test) / 2).astype(int)\n",
        "\n",
        "# Evaluate the performance of each model and the combined model\n",
        "accuracy_bcn = accuracy_score(y_test, bcn_predictions_test)\n",
        "accuracy_svm = accuracy_score(y_test, svm_predictions_test)\n",
        "accuracy_combined = accuracy_score(y_test, combined_predictions_test)\n",
        "\n",
        "print(\"BCN Model Accuracy:\", accuracy_bcn)\n",
        "print(\"SVM Model Accuracy:\", accuracy_svm)\n",
        "print(\"Combined Model Accuracy:\", accuracy_combined)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pcUfiUlcXWcz",
        "outputId": "dfe5aecf-79d1-4788-bde5-eaf0d38d1902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 58ms/step\n",
            "2/2 [==============================] - 0s 30ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of classes has to be greater than one; got 1 class",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-204545736349>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Build and train SVM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msvm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Get SVM predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         sample_weight = np.asarray(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    750\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class (0 for no tumor, 1 for tumor)\n",
        "            labels.append(1 if class_label == 'tumor' else 0)\n",
        "\n",
        "    return np.array(images), np.array(labels, dtype=np.int32)  # Corrected dtype\n",
        "\n",
        "# Load and label the data\n",
        "images, labels = load_and_label_data('/content/drive/MyDrive/brain_tumor_dataset(1)')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to fit the input shape of the BCN model\n",
        "input_shape = (128, 128, 1)  # Adjust based on your actual image size and channels\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0],) + input_shape)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0],) + input_shape)\n",
        "\n",
        "# Build and train the BCN model\n",
        "bcn_model = models.Sequential()\n",
        "bcn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "bcn_model.add(layers.MaxPooling2D((2, 2)))\n",
        "bcn_model.add(layers.Flatten())\n",
        "bcn_model.add(layers.Dense(64, activation='relu'))\n",
        "bcn_model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "bcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bcn_model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Get the BCN model predictions on the test set\n",
        "bcn_predictions_test = bcn_model.predict(X_test_reshaped)\n",
        "bcn_predictions_test = (bcn_predictions_test > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance of the BCN model\n",
        "accuracy_bcn = accuracy_score(y_test, bcn_predictions_test)\n",
        "print(\"BCN Model Accuracy:\", accuracy_bcn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuRedDZFWyUY",
        "outputId": "6439ddd9-e097-4e42-e9d2-f3cd7e3cd176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 4s 442ms/step - loss: 0.1080 - accuracy: 0.9406 - val_loss: 2.6416e-19 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 2s 309ms/step - loss: 1.0818e-14 - accuracy: 1.0000 - val_loss: 6.3354e-26 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 193ms/step - loss: 6.1815e-21 - accuracy: 1.0000 - val_loss: 2.6420e-29 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 193ms/step - loss: 4.9536e-22 - accuracy: 1.0000 - val_loss: 4.3711e-31 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 196ms/step - loss: 2.5298e-23 - accuracy: 1.0000 - val_loss: 5.0384e-32 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 2s 292ms/step - loss: 7.8852e-24 - accuracy: 1.0000 - val_loss: 1.6294e-32 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 2s 208ms/step - loss: 3.4734e-24 - accuracy: 1.0000 - val_loss: 9.0837e-33 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 199ms/step - loss: 2.3622e-24 - accuracy: 1.0000 - val_loss: 6.7302e-33 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 206ms/step - loss: 2.0403e-24 - accuracy: 1.0000 - val_loss: 5.7769e-33 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 204ms/step - loss: 1.8179e-24 - accuracy: 1.0000 - val_loss: 5.3471e-33 - val_accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 39ms/step\n",
            "BCN Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class (0 for no tumor, 1 for tumor)\n",
        "            labels.append(1 if class_label == 'tumor' else 0)\n",
        "\n",
        "    return np.array(images), np.array(labels, dtype=np.int32)  # Corrected dtype\n",
        "\n",
        "# Load and label the data\n",
        "images, labels = load_and_label_data('/content/drive/MyDrive/brain_tumor_dataset(1)')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to fit the input shape of the BCN model\n",
        "input_shape = (128, 128, 1)  # Adjust based on your actual image size and channels\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0],) + input_shape)\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0],) + input_shape)\n",
        "\n",
        "# Build and train the BCN model\n",
        "bcn_model = models.Sequential()\n",
        "bcn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "bcn_model.add(layers.MaxPooling2D((2, 2)))\n",
        "bcn_model.add(layers.Flatten())\n",
        "bcn_model.add(layers.Dense(64, activation='relu'))\n",
        "bcn_model.add(layers.Dense(1, activation='sigmoid'))  # Assuming binary classification\n",
        "bcn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "bcn_model.fit(X_train_reshaped, y_train, epochs=10, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Get the BCN model predictions on the test set\n",
        "bcn_predictions_test = bcn_model.predict(X_test_reshaped)\n",
        "bcn_predictions_test = (bcn_predictions_test > 0.5).astype(int)\n",
        "\n",
        "# Use BCN model predictions as features for SVM\n",
        "X_train_svm = bcn_model.predict(X_train_reshaped)\n",
        "X_test_svm = bcn_model.predict(X_test_reshaped)\n",
        "\n",
        "# Reshape the data for SVM\n",
        "X_train_svm = X_train_svm.reshape((X_train_svm.shape[0], -1))\n",
        "X_test_svm = X_test_svm.reshape((X_test_svm.shape[0], -1))\n",
        "\n",
        "# Build and train SVM model\n",
        "svm_model = make_pipeline(StandardScaler(), SVC())\n",
        "svm_model.fit(X_train_svm, y_train)\n",
        "\n",
        "# Get SVM predictions on the test set\n",
        "svm_predictions_test = svm_model.predict(X_test_svm)\n",
        "\n",
        "# Combine BCN and SVM predictions (you can experiment with other strategies)\n",
        "combined_predictions_test = (bcn_predictions_test + svm_predictions_test) / 2\n",
        "\n",
        "# Convert combined predictions to binary (assuming binary classification)\n",
        "combined_predictions_test = (combined_predictions_test > 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance of the combined model\n",
        "accuracy_combined = accuracy_score(y_test, combined_predictions_test)\n",
        "print(\"Combined Model Accuracy:\", accuracy_combined)\n"
      ],
      "metadata": {
        "id": "pljM4T_rYbTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_data(data_folder):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for folder in os.listdir(data_folder):\n",
        "        if folder in ['yes', 'no']:\n",
        "            label = 1 if folder == 'no' else 0\n",
        "            folder_path = os.path.join(data_folder, folder)\n",
        "\n",
        "            for filename in os.listdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, filename)\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.resize(image, (150, 150))  # Resize images to a consistent size\n",
        "\n",
        "                features.append(image)\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Load and preprocess data\n",
        "data_folder = '/content/drive/MyDrive/brain_tumor_dataset(1)'\n",
        "X, y = load_and_preprocess_data(data_folder)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Build the BCM-CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32), epochs=15, validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "# Extract features from the BCM-CNN model\n",
        "bcm_cnn_features = model.predict(X_train)\n",
        "\n",
        "# Reshape BCM-CNN features for compatibility with SVM\n",
        "bcm_cnn_features_reshaped = bcm_cnn_features.reshape(bcm_cnn_features.shape[0], -1)\n",
        "\n",
        "# Train SVM on BCM-CNN features\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(bcm_cnn_features_reshaped, y_train)\n",
        "\n",
        "# # Save the trained SVM model\n",
        "# svm_model_path = '/content/drive/MyDrive/svm_model.pkl'\n",
        "# joblib.dump(svm_model, svm_model_path)\n",
        "\n",
        "# Extract features from the BCM-CNN model for the test set\n",
        "bcm_cnn_features_test = model.predict(X_test)\n",
        "bcm_cnn_features_test_reshaped = bcm_cnn_features_test.reshape(bcm_cnn_features_test.shape[0], -1)\n",
        "\n",
        "# Make predictions using SVM on BCM-CNN features\n",
        "svm_predictions = svm_model.predict(bcm_cnn_features_test_reshaped)\n",
        "\n",
        "# Evaluate the combined model\n",
        "combined_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "print(f'Combined Model Accuracy: {combined_accuracy * 100:.2f}%')\n",
        "#Combined Model Accuracy: 86.34%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PECeU4IPZG_p",
        "outputId": "84a822e2-f8ea-4a0d-f723-6a9656e65893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "7/7 [==============================] - 7s 917ms/step - loss: 0.8978 - accuracy: 0.6287 - val_loss: 67.9463 - val_accuracy: 0.6078\n",
            "Epoch 2/15\n",
            "7/7 [==============================] - 6s 796ms/step - loss: 0.5370 - accuracy: 0.7426 - val_loss: 121.6342 - val_accuracy: 0.6275\n",
            "Epoch 3/15\n",
            "7/7 [==============================] - 6s 776ms/step - loss: 0.5887 - accuracy: 0.6931 - val_loss: 143.2886 - val_accuracy: 0.6667\n",
            "Epoch 4/15\n",
            "7/7 [==============================] - 6s 991ms/step - loss: 0.5035 - accuracy: 0.7673 - val_loss: 98.1638 - val_accuracy: 0.6667\n",
            "Epoch 5/15\n",
            "7/7 [==============================] - 6s 792ms/step - loss: 0.5261 - accuracy: 0.7772 - val_loss: 154.4043 - val_accuracy: 0.6863\n",
            "Epoch 6/15\n",
            "7/7 [==============================] - 6s 835ms/step - loss: 0.4897 - accuracy: 0.7970 - val_loss: 106.5437 - val_accuracy: 0.6667\n",
            "Epoch 7/15\n",
            "7/7 [==============================] - 6s 914ms/step - loss: 0.5438 - accuracy: 0.7426 - val_loss: 139.8561 - val_accuracy: 0.6863\n",
            "Epoch 8/15\n",
            "7/7 [==============================] - 6s 782ms/step - loss: 0.5174 - accuracy: 0.7673 - val_loss: 92.5566 - val_accuracy: 0.6078\n",
            "Epoch 9/15\n",
            "7/7 [==============================] - 6s 827ms/step - loss: 0.5211 - accuracy: 0.7624 - val_loss: 125.5530 - val_accuracy: 0.6863\n",
            "Epoch 10/15\n",
            "7/7 [==============================] - 7s 873ms/step - loss: 0.5078 - accuracy: 0.7475 - val_loss: 89.4977 - val_accuracy: 0.7059\n",
            "Epoch 11/15\n",
            "7/7 [==============================] - 6s 781ms/step - loss: 0.4979 - accuracy: 0.7525 - val_loss: 147.5729 - val_accuracy: 0.6667\n",
            "Epoch 12/15\n",
            "7/7 [==============================] - 6s 781ms/step - loss: 0.4751 - accuracy: 0.7723 - val_loss: 120.8065 - val_accuracy: 0.7255\n",
            "Epoch 13/15\n",
            "7/7 [==============================] - 6s 866ms/step - loss: 0.4597 - accuracy: 0.8020 - val_loss: 137.6497 - val_accuracy: 0.6863\n",
            "Epoch 14/15\n",
            "7/7 [==============================] - 6s 899ms/step - loss: 0.5433 - accuracy: 0.7624 - val_loss: 63.2947 - val_accuracy: 0.7647\n",
            "Epoch 15/15\n",
            "7/7 [==============================] - 6s 839ms/step - loss: 0.4836 - accuracy: 0.8317 - val_loss: 118.4495 - val_accuracy: 0.6863\n",
            "7/7 [==============================] - 2s 211ms/step\n",
            "2/2 [==============================] - 1s 144ms/step\n",
            "Combined Model Accuracy: 68.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained SVM model\n",
        "svm_model_path = '/content/drive/MyDrive/svm_model(1).pkl'\n",
        "joblib.dump(svm_model, svm_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHH66Gdbx4oB",
        "outputId": "c3201678-3f26-4713-c96f-311c17aa94fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/svm_model(1).pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diEaerTfxspM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZS_6X0mY1lyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJDpCbrS1l04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xk32r1MI1l35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1iuqGG11l7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYvuy2wj1mAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4EST_kIW1mC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6jFr1TB71mEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bk3pQQo1mG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "data_folder = \"/content/drive/MyDrive/brain_tumor_dataset(1)\"\n",
        "\n",
        "labels = []  # List to store labels\n",
        "images_folder = [\"no\", \"yes\"]\n",
        "\n",
        "for folder in images_folder:\n",
        "    folder_path = os.path.join(data_folder, folder)\n",
        "\n",
        "    if folder == \"no\":\n",
        "        label = 0\n",
        "    elif folder == \"yes\":\n",
        "        label = 1\n",
        "\n",
        "    # Iterate over images in the folder and assign labels\n",
        "    for filename in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        labels.append((image_path, label))\n",
        "\n",
        "# Now 'labels' contains tuples with image paths and corresponding labels\n",
        "# Example: [('path/to/healthy_image.jpg', 0), ('path/to/tumor_image.jpg', 1), ...]\n"
      ],
      "metadata": {
        "id": "aMjHUcLxbGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to load and label MRI images\n",
        "def load_and_label_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_label in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, filename)\n",
        "            # Read the image using OpenCV\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "            # Resize the image to a consistent size (adjust as needed)\n",
        "            img = cv2.resize(img, (128, 128))\n",
        "            # Normalize pixel values to be between 0 and 1\n",
        "            img = img / 255.0\n",
        "            # Append the image to the list\n",
        "            images.append(img)\n",
        "            # Assign a label based on the class (0 for no tumor, 1 for tumor)\n",
        "            labels.append(1 if class_label == 'tumor' else 0)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Example usage\n",
        "dataset_path = '/content/drive/MyDrive/brain_tumor_dataset(1)'\n",
        "images, labels = load_and_label_data(dataset_path)\n"
      ],
      "metadata": {
        "id": "0SdR7oBjPjs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}